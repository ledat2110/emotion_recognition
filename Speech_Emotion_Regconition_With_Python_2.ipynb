{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Speech Emotion Regconition With Python 2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tuandat2110/emotion_recognition/blob/master/Speech_Emotion_Regconition_With_Python_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUq3tyimYG9i",
        "colab_type": "text"
      },
      "source": [
        "#Clone git\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWFcf84DX4mc",
        "colab_type": "code",
        "outputId": "cfad60c6-dc9a-4a73-9c4c-96b0d90948d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!git clone https://github.com/tuandat2110/emotion_recognition.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'speech-emotion-recognition'...\n",
            "remote: Enumerating objects: 632, done.\u001b[K\n",
            "remote: Total 632 (delta 0), reused 0 (delta 0), pack-reused 632\u001b[K\n",
            "Receiving objects: 100% (632/632), 31.46 MiB | 36.28 MiB/s, done.\n",
            "Resolving deltas: 100% (112/112), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs6E8RY-YGi9",
        "colab_type": "code",
        "outputId": "1651f70d-5923-43cf-a98e-3ebc39ac8384",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "u'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JdYLde7YIZ9",
        "colab_type": "code",
        "outputId": "721dfa32-87ec-4390-c263-d94fdf59b38b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd speech-emotion-recognition/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/speech-emotion-recognition\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-YQC2BwYPTk",
        "colab_type": "code",
        "outputId": "281eb7b8-bd72-49e3-97bb-8a9be9cc933f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "source": [
        "pip install -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from -r requirements.txt (line 1)) (1.16.4)\n",
            "Requirement already satisfied: h5py>=2.7.1 in /usr/local/lib/python2.7/dist-packages (from -r requirements.txt (line 2)) (2.8.0)\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python2.7/dist-packages (from -r requirements.txt (line 3)) (2.2.4)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python2.7/dist-packages (from -r requirements.txt (line 4)) (1.2.2)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python2.7/dist-packages (from -r requirements.txt (line 5)) (0.0)\n",
            "Collecting speechpy>=2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/8f/12/dbda397a998063d9541d9e149c4f523ed138a48824d20598e37632ba33b1/speechpy-2.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tensorflow>=1.7.0 in /usr/local/lib/python2.7/dist-packages (from -r requirements.txt (line 7)) (1.15.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python2.7/dist-packages (from -r requirements.txt (line 8)) (0.24.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from h5py>=2.7.1->-r requirements.txt (line 2)) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python2.7/dist-packages (from Keras>=2.1.5->-r requirements.txt (line 3)) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from Keras>=2.1.5->-r requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python2.7/dist-packages (from Keras>=2.1.5->-r requirements.txt (line 3)) (3.13)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python2.7/dist-packages (from sklearn->-r requirements.txt (line 5)) (0.20.3)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.7.0->-r requirements.txt (line 7)) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.7.0->-r requirements.txt (line 7)) (1.15.0)\n",
            "Requirement already satisfied: functools32>=3.2.3 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.7.0->-r requirements.txt (line 7)) (3.2.3.post2)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.7.0->-r requirements.txt (line 7)) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.7.0->-r requirements.txt (line 7)) (3.7.1)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1; python_version < \"3.4\" in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.7.0->-r requirements.txt (line 7)) (1.0.post1)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.7.0->-r requirements.txt (line 7)) (2.0.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.7.0->-r requirements.txt (line 7)) (0.33.6)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.7.0->-r requirements.txt (line 7)) (1.11.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.7.0->-r requirements.txt (line 7)) (0.7.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.7.0->-r requirements.txt (line 7)) (2.3.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.7.0->-r requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.7.0->-r requirements.txt (line 7)) (0.1.7)\n",
            "Requirement already satisfied: enum34>=1.1.6; python_version < \"3.4\" in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.7.0->-r requirements.txt (line 7)) (1.1.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.7.0->-r requirements.txt (line 7)) (0.8.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas>=0.24.0->-r requirements.txt (line 8)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/dist-packages (from pandas>=0.24.0->-r requirements.txt (line 8)) (2.5.3)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow>=1.7.0->-r requirements.txt (line 7)) (3.2.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.7.0->-r requirements.txt (line 7)) (41.6.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.7.0->-r requirements.txt (line 7)) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.7.0->-r requirements.txt (line 7)) (3.1.1)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow>=1.7.0->-r requirements.txt (line 7)) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow>=1.7.0->-r requirements.txt (line 7)) (5.4.0)\n",
            "Installing collected packages: speechpy, tensorflow-estimator\n",
            "  Found existing installation: tensorflow-estimator 1.15.0\n",
            "    Uninstalling tensorflow-estimator-1.15.0:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.0\n",
            "Successfully installed speechpy-2.4 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElMDL6uDZFxX",
        "colab_type": "code",
        "outputId": "3747cda1-12d8-4fa7-82d5-45fcd0aafeee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "u'/content/speech-emotion-recognition'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFxD0YSDcmRA",
        "colab_type": "code",
        "outputId": "c975cee7-8a56-4aaa-919b-6d006c6799f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd speechemotionrecognition/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/speech-emotion-recognition/speechemotionrecognition\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAFpxLcFc5Qz",
        "colab_type": "text"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjtNRu6qc2-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "speechemotionrecognition module.\n",
        "Provides a library to perform speech emotion recognition on `emodb` data set\n",
        "\"\"\"\n",
        "import sys\n",
        "from typing import Tuple\n",
        "\n",
        "import numpy\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "__author__ = 'harry-7'\n",
        "__version__ = '1.1'\n",
        "\n",
        "\n",
        "class Model(object):\n",
        "    \"\"\"\n",
        "    Model is the abstract class which determines how a model should be.\n",
        "    Any model inheriting this class should do the following.\n",
        "\n",
        "    1.  Set the model instance variable to the corresponding model class which\n",
        "        which will provide methods `fit` and `predict`.\n",
        "\n",
        "    2.  Should implement the following abstract methods `load_model`,\n",
        "        `save_model` `train` and `evaluate`. These methods provide the\n",
        "        functionality to save the model to the disk, load the model from the\n",
        "        disk and train the model and evaluate the model to return appropriate\n",
        "        measure like accuracy, f1 score, etc.\n",
        "\n",
        "    Attributes:\n",
        "        model (Any): instance variable that holds the model.\n",
        "        save_path (str): path to save the model.\n",
        "        name (str): name of the model.\n",
        "        trained (bool): True if model has been trained, false otherwise.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, save_path = '', name = 'Not Specified'):\n",
        "        \"\"\"\n",
        "        Default constructor for abstract class Model.\n",
        "\n",
        "        Args:\n",
        "            save_path(str): path to save the model to.\n",
        "            name(str): name of the model given as string.\n",
        "\n",
        "        \"\"\"\n",
        "        # Place holder for model\n",
        "        self.model = None\n",
        "        # Place holder on where to save the model\n",
        "        self.save_path = save_path\n",
        "        # Place holder for name of the model\n",
        "        self.name = name\n",
        "        # Model has been trained or not\n",
        "        self.trained = False\n",
        "    \n",
        "    def train(self, x_train, y_train, x_val = None, y_val = None):\n",
        "        \"\"\"\n",
        "        Trains the model with the given training data.\n",
        "\n",
        "        Args:\n",
        "            x_train (numpy.ndarray): samples of training data.\n",
        "            y_train (numpy.ndarray): labels for training data.\n",
        "            x_val (numpy.ndarray): Optional, samples in the validation data.\n",
        "            y_val (numpy.ndarray): Optional, labels of the validation data.\n",
        "\n",
        "        \"\"\"\n",
        "        # This will be specific to model so should be implemented by\n",
        "        # child classes\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def predict(self, samples):\n",
        "        \"\"\"\n",
        "        Predict labels for given data.\n",
        "\n",
        "        Args:\n",
        "            samples (numpy.ndarray): data for which labels need to be predicted\n",
        "\n",
        "        Returns:\n",
        "            list: list of labels predicted for the data.\n",
        "\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for _, sample in enumerate(samples):\n",
        "            results.append(self.predict_one(sample))\n",
        "        return tuple(results)\n",
        "\n",
        "    def predict_one(self, sample):\n",
        "        \"\"\"\n",
        "        Predict label of a single sample. The reason this method exists is\n",
        "        because often we might want to predict label for a single sample.\n",
        "\n",
        "        Args:\n",
        "            sample (numpy.ndarray): Feature vector of the sample that we want to\n",
        "                                    predict the label for.\n",
        "\n",
        "        Returns:\n",
        "            int: returns the label for the sample.\n",
        "        \"\"\"\n",
        "        # This need to be implemented for the child models. The reason is that\n",
        "        # ML models and DL models predict the labels differently.\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def restore_model(self, load_path = None):\n",
        "        \"\"\"\n",
        "        Restore the weights from a saved model and load them to the model.\n",
        "\n",
        "        Args:\n",
        "            load_path (str): Optional, path to load the weights from a given path.\n",
        "\n",
        "        \"\"\"\n",
        "        to_load = load_path or self.save_path\n",
        "        if to_load is None:\n",
        "            sys.stderr.write(\n",
        "                \"Provide a path to load from or save_path of the model\\n\")\n",
        "            sys.exit(-1)\n",
        "        self.load_model(to_load)\n",
        "        self.trained = True\n",
        "\n",
        "    def load_model(self, to_load):\n",
        "        \"\"\"\n",
        "        Load the weights from the given saved model.\n",
        "\n",
        "        Args:\n",
        "            to_load: path containing the saved model.\n",
        "\n",
        "        \"\"\"\n",
        "        # This will be specific to model so should be implemented by\n",
        "        # child classes\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"\n",
        "        Save the model to path denoted by `save_path` instance variable.\n",
        "        \"\"\"\n",
        "        # This will be specific to model so should be implemented by\n",
        "        # child classes\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def evaluate(self, x_test, y_test):\n",
        "        \"\"\"\n",
        "        Evaluate the current model on the given test data.\n",
        "\n",
        "        Predict the labels for test data using the model and print the relevant\n",
        "        metrics like accuracy and the confusion matrix.\n",
        "\n",
        "        Args:\n",
        "            x_test (numpy.ndarray): Numpy nD array or a list like object\n",
        "                                    containing the samples.\n",
        "            y_test (numpy.ndarray): Numpy 1D array or list like object\n",
        "                                    containing the labels for test samples.\n",
        "        \"\"\"\n",
        "        predictions = self.predict(x_test)\n",
        "        print(y_test)\n",
        "        print(predictions)\n",
        "        print('Accuracy:%.3f\\n' % accuracy_score(y_pred=predictions,\n",
        "                                                 y_true=y_test))\n",
        "        print('Confusion matrix:', confusion_matrix(y_pred=predictions,\n",
        "                                                    y_true=y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT-T0HTPd3G9",
        "colab_type": "text"
      },
      "source": [
        "#CNN and LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RR84x-YatIK",
        "colab_type": "code",
        "outputId": "14b36448-23b8-4f3a-fa95-f55e6e6ff9b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "from keras import Sequential\n",
        "from keras.layers import LSTM as KERAS_LSTM, Dense, Dropout, Conv2D, Flatten, \\\n",
        "    BatchNormalization, Activation, MaxPooling2D"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntvGSVficQt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DNN(Model):\n",
        "    \"\"\"\n",
        "    This class is parent class for all Deep neural network models. Any class\n",
        "    inheriting this class should implement `make_default_model` method which\n",
        "    creates a model with a set of hyper parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, num_classes, **params):\n",
        "        \"\"\"\n",
        "        Constructor to initialize the deep neural network model. Takes the input\n",
        "        shape and number of classes and other parameters required for the\n",
        "        abstract class `Model` as parameters.\n",
        "        Args:\n",
        "            input_shape (tuple): shape of the input\n",
        "            num_classes (int): number of different classes ( labels ) in the data.\n",
        "            **params: Additional parameters required by the underlying abstract\n",
        "                class `Model`.\n",
        "        \"\"\"\n",
        "        super(DNN, self).__init__(**params)\n",
        "        self.input_shape = input_shape\n",
        "        self.model = Sequential()\n",
        "        self.make_default_model()\n",
        "        self.model.add(Dense(num_classes, activation='softmax'))\n",
        "        self.model.compile(loss='binary_crossentropy', optimizer='adam',\n",
        "                           metrics=['accuracy'])\n",
        "        print(self.model.summary())\n",
        "        self.save_path = self.save_path or self.name + '_best_model.h5'\n",
        "\n",
        "    def load_model(self, to_load):\n",
        "        \"\"\"\n",
        "        Load the model weights from the given path.\n",
        "        Args:\n",
        "            to_load (str): path to the saved model file in h5 format.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.model.load_weights(to_load)\n",
        "        except:\n",
        "            sys.stderr.write(\"Invalid saved file provided\")\n",
        "            sys.exit(-1)\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"\n",
        "        Save the model weights to `save_path` provided while creating the model.\n",
        "        \"\"\"\n",
        "        self.model.save_weights(self.save_path)\n",
        "\n",
        "    def train(self, x_train, y_train, x_val=None, y_val=None, n_epochs=50):\n",
        "        \"\"\"\n",
        "        Train the model on the given training data.\n",
        "        Args:\n",
        "            x_train (numpy.ndarray): samples of training data.\n",
        "            y_train (numpy.ndarray): labels for training data.\n",
        "            x_val (numpy.ndarray): Optional, samples in the validation data.\n",
        "            y_val (numpy.ndarray): Optional, labels of the validation data.\n",
        "            n_epochs (int): Number of epochs to be trained.\n",
        "        \"\"\"\n",
        "        best_acc = 0\n",
        "        if x_val is None or y_val is None:\n",
        "            x_val, y_val = x_train, y_train\n",
        "        for i in range(n_epochs):\n",
        "            # Shuffle the data for each epoch in unison inspired\n",
        "            # from https://stackoverflow.com/a/4602224\n",
        "            p = np.random.permutation(len(x_train))\n",
        "            x_train = x_train[p]\n",
        "            y_train = y_train[p]\n",
        "            self.model.fit(x_train, y_train, batch_size=32, epochs=1)\n",
        "            loss, acc = self.model.evaluate(x_val, y_val)\n",
        "            if acc > best_acc:\n",
        "                best_acc = acc\n",
        "        self.trained = True\n",
        "\n",
        "    def predict_one(self, sample):\n",
        "        if not self.trained:\n",
        "            sys.stderr.write(\n",
        "                \"Model should be trained or loaded before doing predict\\n\")\n",
        "            sys.exit(-1)\n",
        "        return np.argmax(self.model.predict(np.array([sample])))\n",
        "\n",
        "    def make_default_model(self):\n",
        "        \"\"\"\n",
        "        Make the model with default hyper parameters\n",
        "        \"\"\"\n",
        "        # This has to be implemented by child classes. The reason is that the\n",
        "        # hyper parameters depends on the model.\n",
        "        raise NotImplementedError()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysvlVtw5ePOK",
        "colab_type": "text"
      },
      "source": [
        "#CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExulnLDFeNsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(DNN):\n",
        "    \"\"\"\n",
        "    This class handles CNN for speech emotion recognitions\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **params):\n",
        "        params['name'] = 'CNN'\n",
        "        super(CNN, self).__init__(**params)\n",
        "\n",
        "    def make_default_model(self):\n",
        "        \"\"\"\n",
        "        Makes a CNN keras model with the default hyper parameters.\n",
        "        \"\"\"\n",
        "        self.model.add(Conv2D(8, (13, 13),\n",
        "                              input_shape=(\n",
        "                                  self.input_shape[0], self.input_shape[1], 1)))\n",
        "        self.model.add(BatchNormalization(axis=-1))\n",
        "        self.model.add(Activation('relu'))\n",
        "        self.model.add(Conv2D(8, (13, 13)))\n",
        "        self.model.add(BatchNormalization(axis=-1))\n",
        "        self.model.add(Activation('relu'))\n",
        "        self.model.add(MaxPooling2D(pool_size=(2, 1)))\n",
        "        self.model.add(Conv2D(8, (13, 13)))\n",
        "        self.model.add(BatchNormalization(axis=-1))\n",
        "        self.model.add(Activation('relu'))\n",
        "        self.model.add(Conv2D(8, (2, 2)))\n",
        "        self.model.add(BatchNormalization(axis=-1))\n",
        "        self.model.add(Activation('relu'))\n",
        "        self.model.add(MaxPooling2D(pool_size=(2, 1)))\n",
        "        self.model.add(Flatten())\n",
        "        self.model.add(Dense(64))\n",
        "        self.model.add(BatchNormalization())\n",
        "        self.model.add(Activation('relu'))\n",
        "        self.model.add(Dropout(0.2))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEf6XNo6eQ1C",
        "colab_type": "text"
      },
      "source": [
        "#LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7Fd9Hy-eB7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM(DNN):\n",
        "    \"\"\"\n",
        "    This class handles CNN for speech emotion recognitions\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **params):\n",
        "        params['name'] = 'LSTM'\n",
        "        super(LSTM, self).__init__(**params)\n",
        "\n",
        "    def make_default_model(self):\n",
        "        \"\"\"\n",
        "        Makes the LSTM model with keras with the default hyper parameters.\n",
        "        \"\"\"\n",
        "        self.model.add(\n",
        "            KERAS_LSTM(128,\n",
        "                       input_shape=(self.input_shape[0], self.input_shape[1])))\n",
        "        self.model.add(Dropout(0.5))\n",
        "        self.model.add(Dense(32, activation='relu'))\n",
        "        self.model.add(Dense(16, activation='tanh'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mT3A6QWeCf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVOmBi2DfQnG",
        "colab_type": "text"
      },
      "source": [
        "#Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ogo9thWvfdfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "author: harry-7\n",
        "This file contains functions to read the data files from the given folders and\n",
        "generate Mel Frequency Cepestral Coefficients features for the given audio\n",
        "files as training samples.\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wav\n",
        "from speechpy.feature import mfcc\n",
        "\n",
        "\n",
        "mean_signal_length = 32000  # Empirically calculated for the given data set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21nEg5e3fduT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_feature_vector_from_mfcc(file_path, flatten, mfcc_len = 39):\n",
        "    \"\"\"\n",
        "    Make feature vector from MFCC for the given wav file.\n",
        "    Args:\n",
        "        file_path (str): path to the .wav file that needs to be read.\n",
        "        flatten (bool) : Boolean indicating whether to flatten mfcc obtained.\n",
        "        mfcc_len (int): Number of cepestral co efficients to be consider.\n",
        "    Returns:\n",
        "        numpy.ndarray: feature vector of the wav file made from mfcc.\n",
        "    \"\"\"\n",
        "    fs, signal = wav.read(file_path)\n",
        "    s_len = len(signal)\n",
        "    # pad the signals to have same size if lesser than required\n",
        "    # else slice them\n",
        "    if s_len < mean_signal_length:\n",
        "        pad_len = mean_signal_length - s_len\n",
        "        pad_rem = pad_len % 2\n",
        "        pad_len //= 2\n",
        "        signal = np.pad(signal, (pad_len, pad_len + pad_rem),\n",
        "                        'constant', constant_values=0)\n",
        "    else:\n",
        "        pad_len = s_len - mean_signal_length\n",
        "        pad_len //= 2\n",
        "        signal = signal[pad_len:pad_len + mean_signal_length]\n",
        "    mel_coefficients = mfcc(signal, fs, num_cepstral=mfcc_len)\n",
        "    if flatten:\n",
        "        # Flatten the data\n",
        "        mel_coefficients = np.ravel(mel_coefficients)\n",
        "    #def get_data(data_path, flatten = True, mfcc_len = 39, class_labels):\n",
        "    return mel_coefficients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMEWvIFVfdp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(data_path, flatten = True, mfcc_len = 39, class_labels = None):\n",
        "    \"\"\"Extract data for training and testing.\n",
        "    1. Iterate through all the folders.\n",
        "    2. Read the audio files in each folder.\n",
        "    3. Extract Mel frequency cepestral coefficients for each file.\n",
        "    4. Generate feature vector for the audio files as required.\n",
        "    Args:\n",
        "        data_path (str): path to the data set folder\n",
        "        flatten (bool): Boolean specifying whether to flatten the data or not.\n",
        "        mfcc_len (int): Number of mfcc features to take for each frame.\n",
        "        class_labels (tuple): class labels that we care about.\n",
        "    Returns:\n",
        "        Tuple[numpy.ndarray, numpy.ndarray]: Two numpy arrays, one with mfcc and\n",
        "        other with labels.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    labels = []\n",
        "    names = []\n",
        "    cur_dir = os.getcwd()\n",
        "    sys.stderr.write('curdir: %s\\n' % cur_dir)\n",
        "    os.chdir(data_path)\n",
        "    for i, directory in enumerate(class_labels):\n",
        "        sys.stderr.write(\"started reading folder %s\\n\" % directory)\n",
        "        os.chdir(directory)\n",
        "        for filename in os.listdir('.'):\n",
        "            filepath = os.getcwd() + '/' + filename\n",
        "            feature_vector = get_feature_vector_from_mfcc(file_path=filepath,\n",
        "                                                          mfcc_len=mfcc_len,\n",
        "                                                          flatten=flatten)\n",
        "            data.append(feature_vector)\n",
        "            labels.append(i)\n",
        "            names.append(filename)\n",
        "        sys.stderr.write(\"ended reading folder %s\\n\" % directory)\n",
        "        os.chdir('..')\n",
        "    os.chdir(cur_dir)\n",
        "    return np.array(data), np.array(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Cn29deWfdmK",
        "colab_type": "code",
        "outputId": "7d13394b-6a98-4c99-b65d-45e8cb95ceae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd ../.."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FleFDFf2fdju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr1iVFSbfdcz",
        "colab_type": "code",
        "outputId": "22b4b2da-4926-428a-ebe0-aade9c8c56dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "gdd.download_file_from_google_drive(file_id='1VqGqFcP9Wtjt0rf9j4UUq5fgMo9TRCnx', dest_path='./data.zip')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 1VqGqFcP9Wtjt0rf9j4UUq5fgMo9TRCnx into ./Handout... Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yaPNcIrsA5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = 'data'\n",
        "os.listdir(data_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km1PJmuDeXWl",
        "colab_type": "text"
      },
      "source": [
        "#Reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0Ev0i1te5bs",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   Link: https://github.com/harry-7/speech-emotion-recognition\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBruajTZeYlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
